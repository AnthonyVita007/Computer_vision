{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d51f52b-efc1-4eb7-9773-7988114b6884",
   "metadata": {},
   "source": [
    "# Implementazione della Rilevazione in Tempo Reale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694a6aa-5b14-4dce-ade3-1a23c27b364b",
   "metadata": {},
   "source": [
    "## Settings\n",
    "In questo script, implementiamo la logica per utilizzare il nostro modello addestrato su un flusso video live proveniente da una webcam. Il processo si articola in questi punti:\n",
    "1.  **Caricamento dei Modelli**: Carichiamo sia il nostro modello di classificazione delle emozioni (`.h5` o `.keras`) sia il classificatore Haar di OpenCV per la rilevazione dei volti (`.xml`).\n",
    "2.  **Acquisizione Video**: Avviamo la cattura del video dalla webcam.\n",
    "3.  **Ciclo di Elaborazione**: In un ciclo continuo, leggiamo ogni fotogramma del video, rileviamo i volti, e per ogni volto trovato applichiamo il nostro modello per predirne l'emozione.\n",
    "4.  **Visualizzazione**: Mostriamo a schermo il video con i riquadri attorno ai volti e l'etichetta dell'emozione predetta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277ffde-f5f1-4dd2-8f8d-5faada7a35a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitaa\\anaconda3\\envs\\cv\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "C:\\Users\\vitaa\\anaconda3\\envs\\cv\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.5 when it was built against 1.14.6, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vitaa\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\vitaa\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\vitaa\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\vitaa\\anaconda3\\envs\\cv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- 1. Caricamento delle Risorse ---\n",
    "\n",
    "# Caricamento del modello per il riconoscimento delle emozioni\n",
    "model = tf.keras.models.load_model('frank_emotion_detector_model.keras')\n",
    "\n",
    "# Caricamento del classificatore a cascata Haar per la rilevazione dei volti\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Definiamo le etichette delle emozioni in ordine, come nel training\n",
    "emotion_labels = [\"Rabbia\", \"Disgusto\", \"Paura\", \"Felicita'\", \"Neutralita'\", \"Tristezza\", \"Sorpresa\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d14cc-04d4-40e6-91b5-f918608a1bb7",
   "metadata": {},
   "source": [
    "## Avviamento Webcam e il Ciclo Principale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f39da-543c-4d90-a06e-daf2f1de3dc0",
   "metadata": {},
   "source": [
    "Ora inizializziamo la webcam e creiamo un ciclo while che continuerà ad essere eseguito finché non decideremo di chiudere il programma. Questo ciclo è il cuore della nostra applicazione in tempo reale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945219a-a155-41cc-9817-2f5f503fabaa",
   "metadata": {},
   "source": [
    "Il seguente blocco di codice contiene il ciclo principale dell'applicazione. Per ogni fotogramma catturato dalla webcam:\n",
    "\n",
    "1. Viene convertito in scala di grigi, formato ottimale per il classificatore Haar.\n",
    "2. Il classificatore `detectMultiScale` rileva tutti i volti presenti, restituendo le loro coordinate.\n",
    "3. Per ogni volto trovato (chiamato ROI - *Region of Interest*), eseguiamo la pre-elaborazione necessaria per il nostro modello (resize a 224x224, aggiunta della dimensione del batch) e lo passiamo al modello per ottenere una predizione.\n",
    "4. Infine, disegniamo un rettangolo attorno al volto e scriviamo l'emozione predetta sul fotogramma originale, che viene poi mostrato a schermo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd3047f-7af4-4027-8660-9177566defd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Avvio della Webcam e Ciclo Principale ---\n",
    "\n",
    "# Inizializza la cattura video dalla webcam predefinita (indice 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Legge un singolo fotogramma (frame) dalla webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Converte il frame in scala di grigi per il face detector\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Rileva i volti nel frame in scala di grigi\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Itera su ogni volto rilevato\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Estrae la regione di interesse (ROI - il volto) dal frame a colori\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # --- Pre-elaborazione del volto per il nostro modello ---\n",
    "        # 1. Ridimensiona il volto a 224x224, come richiesto dal modello\n",
    "        resized_face = cv2.resize(face_roi, (224, 224))\n",
    "        # 2. Aggiunge una dimensione per il \"batch\" (il modello si aspetta un gruppo di immagini)\n",
    "        input_image = np.expand_dims(resized_face, axis=0)\n",
    "\n",
    "        # --- Predizione (Inferenza) ---\n",
    "        # Usa il modello per predire l'emozione. La funzione predict è velocissima.\n",
    "        prediction = model.predict(input_image)\n",
    "\n",
    "        # Ottiene l'indice dell'emozione con la probabilità più alta\n",
    "        predicted_class_index = np.argmax(prediction)\n",
    "\n",
    "        # Ottiene l'etichetta testuale corrispondente\n",
    "        predicted_emotion = emotion_labels[predicted_class_index]\n",
    "\n",
    "        # --- Visualizzazione ---\n",
    "        # Disegna un rettangolo verde attorno al volto rilevato\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        # Scrive l'etichetta dell'emozione predetta sopra il rettangolo\n",
    "        cv2.putText(frame, predicted_emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Mostra il frame finale con le rilevazioni in una finestra\n",
    "    cv2.imshow('Riconoscimento Emozioni', frame)\n",
    "\n",
    "    # Interrompe il ciclo se viene premuto il tasto 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- 3. Rilascio delle Risorse ---\n",
    "# Rilascia la webcam e chiude tutte le finestre di OpenCV\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
